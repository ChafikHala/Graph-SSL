{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fed086a",
   "metadata": {},
   "source": [
    "---\n",
    "### SimCLR\n",
    "\n",
    "##### 1. Overview\n",
    "\n",
    "**SimCLR (Simple Contrastive Learning of Representations)** is a self-supervised learning framework that trains neural networks to learn meaningful representations **without labels** by solving a *contrastive prediction task*.\n",
    "\n",
    "The key objective is:\n",
    "\n",
    "> Learn embeddings in which two augmented views of the **same sample** are close together, while embeddings of **different samples** are far apart.\n",
    "\n",
    "This is achieved using a contrastive loss known as **NT-Xent** (Normalized Temperature-scaled Cross-Entropy), which is a specific instantiation of the **InfoNCE** objective.\n",
    "\n",
    "\n",
    "\n",
    "##### 2. Data Pipeline\n",
    "\n",
    "For each data point $x \\sim \\mathcal{D}$:\n",
    "\n",
    "1. Two stochastic augmentations are sampled:\n",
    "   $$\n",
    "   x_i = t_a(x), \\qquad x_j = t_b(x).\n",
    "   $$\n",
    "\n",
    "2. Both views are passed through:\n",
    "   - an **encoder** $ f_\\theta $,\n",
    "   - a **projection head** $ g_\\phi $,\n",
    "\n",
    "   yielding:\n",
    "   $$\n",
    "   h = f_\\theta(x), \\qquad\n",
    "   z = g_\\phi(h), \\qquad\n",
    "   \\tilde z = \\frac{z}{\\|z\\|_2}.\n",
    "   $$\n",
    "\n",
    "3. $\\tilde z $ is the representation used for the contrastive loss.\n",
    "\n",
    "\n",
    "##### 3. Positive and Negative Pairs\n",
    "\n",
    "With a minibatch of size $ N $ original samples:\n",
    "\n",
    "- We produce $2N $ augmented views.\n",
    "- For each embedding $ \\tilde z_i $:\n",
    "  - the paired view $ \\tilde z_j $ of the same data sample is the **positive example**,\n",
    "  - the remaining $2N - 2 $ embeddings are treated as **negatives**.\n",
    "\n",
    "No explicit negative sampling is required — negatives come from the batch.\n",
    "\n",
    "\n",
    "\n",
    "##### 4. The NT-Xent (SimCLR) Loss\n",
    "\n",
    "The similarity function is cosine similarity with temperature scaling:\n",
    "\n",
    "$$\n",
    "s(\\tilde z_i, \\tilde z_k)\n",
    "=\n",
    "\\frac{\\tilde z_i^\\top \\tilde z_k}{\\tau},\n",
    "$$\n",
    "\n",
    "where $ \\tau > 0 $ is the temperature hyperparameter.\n",
    "\n",
    "For anchor  $i$ and its positive partner $ j $, the normalized temperature-scaled cross-entropy (**NT-Xent**) loss is:\n",
    "\n",
    "$$\n",
    "\\ell(i,j)\n",
    "=\n",
    "-\\log\n",
    "\\frac{\n",
    "\\exp\\!\\big(s(\\tilde z_i,\\tilde z_j)\\big)\n",
    "}{\n",
    "\\sum_{k=1}^{2N}\n",
    "\\mathbf{1}_{[k \\neq i]}\n",
    "\\exp\\!\\big(s(\\tilde z_i,\\tilde z_k)\\big)\n",
    "}.\n",
    "$$\n",
    "\n",
    "**Properties**\n",
    "\n",
    "- The **positive example is included in the denominator**.\n",
    "- The denominator forms a softmax over all candidates except the trivial self-pair $ i $.\n",
    "- Each embedding acts as an anchor once; the loss is **symmetrized**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{SimCLR}}\n",
    "=\n",
    "\\frac{1}{2N}\n",
    "\\sum_{(i,j)}\n",
    "\\big( \\ell(i,j) + \\ell(j,i) \\big).\n",
    "$$\n",
    "\n",
    "This is mathematically equivalent to performing a $2N-1$-way classification task:\n",
    "\n",
    "Given anchor $ i $, predict which candidate  $k \\neq i $ is its true positive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f28a561a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\halac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 020 | Loss = 7.1274\n",
      "Epoch 040 | Loss = 6.9606\n",
      "Epoch 060 | Loss = 6.8973\n",
      "Epoch 080 | Loss = 6.8680\n",
      "Epoch 100 | Loss = 6.8393\n",
      "Epoch 120 | Loss = 6.8237\n",
      "Epoch 140 | Loss = 6.8112\n",
      "Epoch 160 | Loss = 6.7968\n",
      "Epoch 180 | Loss = 6.7886\n",
      "Epoch 200 | Loss = 6.7779\n",
      "Epoch 220 | Loss = 6.7767\n",
      "Epoch 240 | Loss = 6.7713\n",
      "Epoch 260 | Loss = 6.7627\n",
      "Epoch 280 | Loss = 6.7559\n",
      "Epoch 300 | Loss = 6.7564\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import dropout_edge\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "dataset = Planetoid(root=\"C:/Users/halac/Graph-SSL/data\", name=\"Cora\")\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "num_nodes  = data.num_nodes\n",
    "num_feats  = data.num_features\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "\n",
    "# Graph Augmentations (SimCLR views)\n",
    "\n",
    "\n",
    "def random_edge_dropout(edge_index, p=0.2):\n",
    "    edge_index, _ = dropout_edge(edge_index, p=p)\n",
    "    return edge_index\n",
    "\n",
    "def random_feature_mask(x, p=0.2):\n",
    "    mask = torch.rand_like(x) > p\n",
    "    return x * mask\n",
    "\n",
    "\n",
    "def augment_graph(data, edge_p=0.2, feat_p=0.2):\n",
    "    \"\"\"\n",
    "    Create a stochastic augmented view of the full graph.\n",
    "    \"\"\"\n",
    "    x = random_feature_mask(data.x, p=feat_p)\n",
    "    edge_index = random_edge_dropout(data.edge_index, p=edge_p)\n",
    "\n",
    "    return x, edge_index\n",
    "\n",
    "\n",
    "\n",
    "# GCN Encoder\n",
    "\n",
    "\n",
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=128, out_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(h, edge_index)\n",
    "        return h\n",
    "\n",
    "\n",
    "\n",
    "# Projection Head (SimCLR)\n",
    "\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, h):\n",
    "        return self.mlp(h)\n",
    "\n",
    "\n",
    "\n",
    "# NT-Xent Contrastive Loss (SimCLR)\n",
    "\n",
    "\n",
    "def nt_xent_loss(z1, z2, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Node-wise SimCLR loss.\n",
    "    \"\"\"\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "\n",
    "    N = z1.size(0)\n",
    "\n",
    "    z = torch.cat([z1, z2], dim=0)     # (2N, d)\n",
    "    sim = torch.matmul(z, z.T)         # cosine similarity\n",
    "\n",
    "    # positive pairs along diagonal\n",
    "    pos = torch.cat([\n",
    "        torch.diag(sim, N),\n",
    "        torch.diag(sim, -N)\n",
    "    ], dim=0)\n",
    "\n",
    "    neg_mask = ~torch.eye(2 * N, device=device, dtype=torch.bool)\n",
    "\n",
    "    negatives = sim[neg_mask].view(2 * N, -1)\n",
    "\n",
    "    logits = torch.cat([pos.unsqueeze(1), negatives], dim=1)\n",
    "    logits /= temperature\n",
    "\n",
    "    labels = torch.zeros(2 * N, dtype=torch.long, device=device)\n",
    "\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "\n",
    "\n",
    "# Model\n",
    "\n",
    "\n",
    "encoder = GCNEncoder(num_feats).to(device)\n",
    "projector = ProjectionHead(128).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(projector.parameters()),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Training Loop (Graph SimCLR)\n",
    "\n",
    "epochs = 300\n",
    "temperature = 0.5\n",
    "\n",
    "encoder.train()\n",
    "projector.train()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "    # Two random augmented graph views\n",
    "    x1, edge1 = augment_graph(data)\n",
    "    x2, edge2 = augment_graph(data)\n",
    "\n",
    "    # Encode\n",
    "    h1 = encoder(x1, edge1)\n",
    "    h2 = encoder(x2, edge2)\n",
    "\n",
    "    # Project\n",
    "    z1 = projector(h1)\n",
    "    z2 = projector(h2)\n",
    "\n",
    "    # Contrastive loss\n",
    "    loss = nt_xent_loss(z1, z2, temperature)\n",
    "\n",
    "    # Optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch:03d} | Loss = {loss.item():.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bd2fde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test accuracy: 0.7910\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = encoder(data.x, data.edge_index)\n",
    "\n",
    "# Train linear classifier on labeled train mask\n",
    "clf = nn.Linear(128, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(clf.parameters(), lr=0.01)\n",
    "\n",
    "for _ in range(200):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = clf(embeddings[data.train_mask])\n",
    "    loss = F.cross_entropy(out, data.y[data.train_mask])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "clf.eval()\n",
    "pred = clf(embeddings).argmax(dim=1)\n",
    "\n",
    "acc = (\n",
    "    pred[data.test_mask] ==\n",
    "    data.y[data.test_mask]\n",
    ").float().mean().item()\n",
    "\n",
    "print(f\"✅ Test accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d071762",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd872bc",
   "metadata": {},
   "source": [
    "## MixCo\n",
    "SimCLR is extended by introducing **mixup** and **soft positives**.\n",
    "\n",
    "#### Mixup\n",
    "\n",
    "For anchor $z_i$:\n",
    "\n",
    "1. Keep **true positive** $z_j$.\n",
    "2. Sample a random partner $z_q$.\n",
    "3. Sample a coefficient $\\lambda \\sim \\text{Beta}(\\alpha,\\alpha)$.\n",
    "4. Create a mixed representation:\n",
    "\n",
    "$z_{\\text{mix}} = \\lambda z_j + (1-\\lambda) z_q.$\n",
    "\n",
    "\n",
    "\n",
    "#### Soft targets\n",
    "\n",
    "The anchor now has **two positives instead of one**, with weights (label smoothing?):\n",
    "\n",
    "$y_{ik} =\n",
    "\\begin{cases}\n",
    "\\lambda & k = j \\\\\n",
    "1 - \\lambda & k = q \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "All other embeddings remain negatives.\n",
    "\n",
    "\n",
    "\n",
    "#### Soft NT-Xent loss\n",
    "\n",
    "Using the same probabilities:\n",
    "\n",
    "$p_{ik} = \\frac{\\exp(s(z_i,z_k))}{\\sum_{m \\neq i} \\exp(s(z_i,z_m))},$\n",
    "\n",
    "the MixCo loss for anchor $i$ becomes:\n",
    "\n",
    "$L_i^{\\text{MixCo}}\n",
    "= -\\sum_k y_{ik}\\log p_{ik}.$\n",
    "\n",
    "\n",
    "\n",
    "#### Key differences\n",
    "\n",
    "| Method | Positives per anchor | Target type | Effect |\n",
    "|--------|------------------------|----------------|---------|\n",
    "| **SimCLR** | 1 (paired view) | Hard (0 or 1) | Strong pull to one positive, strong repulsion to all others |\n",
    "| **MixCo** | 2 (paired view + random mix partner) | Soft weights via $\\lambda$ | Reduced false-negative repulsion, smoother gradients, improved stability |\n",
    "\n",
    "\n",
    "\n",
    "#### About $\\lambda$\n",
    "\n",
    "- $\\lambda$ is **not fixed**.\n",
    "- For each mixed pair and each forward pass:\n",
    "\n",
    "$\\lambda \\sim \\text{Beta}(\\alpha,\\alpha)$\n",
    "\n",
    "- $\\alpha$ is the **hyperparameter** controlling the distribution:\n",
    "  - Larger $\\alpha$ → $\\lambda$ concentrates near $0.5$.\n",
    "  - Smaller $\\alpha$ → more extreme values (near 0 or 1).\n",
    "\n",
    "In practice $\\alpha \\approx 1$ or greater is used so that:\n",
    "\n",
    "- Very small $\\lambda$ values are rare.\n",
    "- The true positive usually retains significant weight.\n",
    "\n",
    "Occasional small $\\lambda$ acts as **benign regularization**, not destructive noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b714315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MixCo Loss (Graph MixCo)\n",
    "\n",
    "\n",
    "def mixco_loss(z1, z2, temperature=0.5, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Node-wise MixCo loss.\n",
    "    z1, z2: node embeddings from two augmented views\n",
    "    \"\"\"\n",
    "\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "\n",
    "    N, d = z1.shape\n",
    "\n",
    "    \n",
    "    # choose random mix partner for each node\n",
    "    \n",
    "    perm = torch.randperm(N, device=device)\n",
    "\n",
    "    z2_perm = z2[perm]\n",
    "\n",
    "    \n",
    "    # sample mixing coefficient lambda per node\n",
    "    \n",
    "    beta = torch.distributions.Beta(alpha, alpha)\n",
    "    lam = beta.sample((N,)).to(device)              # (N,)\n",
    "    lam = lam.view(N, 1)\n",
    "\n",
    "    \n",
    "    # create mixed positive representations\n",
    "    \n",
    "    z_mix = lam * z2 + (1. - lam) * z2_perm          # (N,d)\n",
    "\n",
    "    \n",
    "    # Step 4: similarity scores against all candidates\n",
    "    \n",
    "    z_all = torch.cat([z2, z2_perm], dim=0)          # (2N,d)\n",
    "\n",
    "    sim = torch.matmul(z_mix, z_all.T) / temperature  # (N,2N)\n",
    "\n",
    "    \n",
    "    # construct soft targets\n",
    "    # targets[i, j] = lambda_i\n",
    "    # targets[i, perm[i]+N] = (1-lambda_i)\n",
    "    \n",
    "    targets = torch.zeros(N, 2*N, device=device)\n",
    "\n",
    "    ij = torch.arange(N, device=device)\n",
    "\n",
    "    # true positives (z2)\n",
    "    targets[ij, ij] = lam.squeeze()\n",
    "\n",
    "    # mixed positives (permuted z2)\n",
    "    targets[ij, perm + N] = (1. - lam).squeeze()\n",
    "\n",
    "    # soft cross-entropy\n",
    "    log_probs = F.log_softmax(sim, dim=1)\n",
    "\n",
    "    loss = -(targets * log_probs).sum(dim=1).mean()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64bcef38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020 | Loss = 7.5789\n",
      "Epoch 040 | Loss = 7.3942\n",
      "Epoch 060 | Loss = 7.3576\n",
      "Epoch 080 | Loss = 7.3073\n",
      "Epoch 100 | Loss = 7.2677\n",
      "Epoch 120 | Loss = 7.3000\n",
      "Epoch 140 | Loss = 7.2399\n",
      "Epoch 160 | Loss = 7.3007\n",
      "Epoch 180 | Loss = 7.2183\n",
      "Epoch 200 | Loss = 7.2222\n",
      "Epoch 220 | Loss = 7.2636\n",
      "Epoch 240 | Loss = 7.2283\n",
      "Epoch 260 | Loss = 7.2715\n",
      "Epoch 280 | Loss = 7.2086\n",
      "Epoch 300 | Loss = 7.2205\n"
     ]
    }
   ],
   "source": [
    "encoder = GCNEncoder(num_feats).to(device)\n",
    "projector = ProjectionHead(128).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(projector.parameters()),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Training Loop (Graph MixCo)\n",
    "\n",
    "epochs = 300\n",
    "temperature = 0.2\n",
    "\n",
    "encoder.train()\n",
    "projector.train()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "    # Two random augmented graph views\n",
    "    x1, edge1 = augment_graph(data)\n",
    "    x2, edge2 = augment_graph(data)\n",
    "\n",
    "    # Encode\n",
    "    h1 = encoder(x1, edge1)\n",
    "    h2 = encoder(x2, edge2)\n",
    "\n",
    "    # Project\n",
    "    z1 = projector(h1)\n",
    "    z2 = projector(h2)\n",
    "\n",
    "    # Contrastive loss\n",
    "    loss = mixco_loss(z1, z2, temperature, alpha = 2.0)\n",
    "\n",
    "    # Optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch:03d} | Loss = {loss.item():.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "585d60fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test accuracy: 0.6930\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = encoder(data.x, data.edge_index)\n",
    "\n",
    "# Train linear classifier on labeled train mask\n",
    "clf = nn.Linear(128, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(clf.parameters(), lr=0.01)\n",
    "\n",
    "for _ in range(200):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = clf(embeddings[data.train_mask])\n",
    "    loss = F.cross_entropy(out, data.y[data.train_mask])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "clf.eval()\n",
    "pred = clf(embeddings).argmax(dim=1)\n",
    "\n",
    "acc = (\n",
    "    pred[data.test_mask] ==\n",
    "    data.y[data.test_mask]\n",
    ").float().mean().item()\n",
    "\n",
    "print(f\"✅ Test accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632c062d",
   "metadata": {},
   "source": [
    "### Problem Addressed by **MoCHi**\n",
    "\n",
    "In standard contrastive learning methods such as SimCLR, each anchor embedding is compared against a large number of negative samples, and all negatives are treated equally in the loss. \n",
    "\n",
    "In practice, however, most of these negatives are **easy negatives**: they are already far away from the anchor in representation space and therefore contribute almost no gradient to the training objective. Only a small fraction of negatives are **hard negatives**; negatives actually teach the model how to separate similar-but-different samples and generate informative gradients that refine the decision boundary. Because hard negatives occur rarely by chance, contrastive learning becomes inefficient: it requires very **large** batch sizes or large memory queues to encounter enough useful negatives. \n",
    "\n",
    "**MoCHi addresses this inefficiency by actively generating hard negatives instead of relying on random sampling, ensuring that each training batch contains challenging and informative contrastive examples.**\n",
    "\n",
    "### MoCHi: Algorithm and Updated Loss\n",
    "\n",
    "\n",
    "\n",
    "#### Algorithm (per training step)\n",
    "\n",
    "For each anchor embedding $z_i$:\n",
    "\n",
    "1. **Compute similarities** to all negatives in the batch / memory bank:\n",
    "   \n",
    "   $s_{ik} = z_i^\\top z_k.$\n",
    "\n",
    "2. **Select hard negatives**:\n",
    "   - Rank negatives by similarity.\n",
    "   - Choose two from the highest-similarity set:\n",
    "     \n",
    "     $z_p, z_q \\in \\text{HardNeg}(i).$\n",
    "\n",
    "3. **Mix hard negatives**:\n",
    "   \n",
    "   Sample $\\lambda \\sim \\text{Beta}(\\alpha,\\alpha)$ and form\n",
    "\n",
    "   $z_{\\text{mix}} = \\lambda z_p + (1-\\lambda) z_q.$\n",
    "\n",
    "4. **Add the mixed sample to the negative set** for anchor $z_i$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Updated Contrastive Loss\n",
    "\n",
    "The loss keeps the standard **InfoNCE / NT-Xent form**, but with an **augmented negative set**.\n",
    "\n",
    "##### Similarity\n",
    "\n",
    "$s_{ik} = \\frac{z_i^\\top z_k}{\\tau}$\n",
    "\n",
    "\n",
    "\n",
    "#### MoCHi loss for anchor $i$\n",
    "\n",
    "$$\n",
    "L_i^{\\text{MoCHi}}\n",
    "=\n",
    "- \\log\n",
    "\\frac{\n",
    "\\exp(s_{ij})\n",
    "}{\n",
    "\\exp(s_{ij})\n",
    "+\n",
    "\\sum_{k \\in \\mathcal N_i}\n",
    "\\exp(s_{ik})\n",
    "+\n",
    "\\exp(s_{i,\\text{mix}})\n",
    "}.\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $j$ is the true positive for anchor $i$,\n",
    "- $\\mathcal N_i$ is the set of normal negatives,\n",
    "- $z_{\\text{mix}}$ is the **synthetic hard negative**.\n",
    "\n",
    "\n",
    "\n",
    "#### What changed vs SimCLR\n",
    "\n",
    "| Element | SimCLR | MoCHi |\n",
    "|--------|---------|-------|\n",
    "| Positives | 1 hard positive | Same |\n",
    "| Negatives | Random batch negatives | Random + **synthetic hard negatives** |\n",
    "| Mixing | Not used | **Mix hard negatives** |\n",
    "| Loss type | NT-Xent / InfoNCE | **Same formula, larger denominator** |\n",
    "| Main benefit | Needs huge batches | **Creates hard negatives explicitly** |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37607f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MoCHi Loss (Hard Negative Mixing for Graph SimCLR)\n",
    "\n",
    "\n",
    "def mochi_loss(z1, z2, temperature=0.5, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Node-wise MoCHi loss.\n",
    "    For each anchor z1[i], we:\n",
    "        - use z2[i] as the positive\n",
    "        - find the two hardest negatives among z2[j!=i]\n",
    "        - mix them to create a synthetic negative\n",
    "        - add it to the NT-Xent denominator\n",
    "    \"\"\"\n",
    "    \n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "\n",
    "    N, d = z1.size()\n",
    "\n",
    "    # similarity matrix between anchors and candidate negatives\n",
    "    sim = torch.matmul(z1, z2.T)   # (N, N)\n",
    "\n",
    "    beta = torch.distributions.Beta(alpha, alpha)\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i in range(N):\n",
    "\n",
    "        # Positive\n",
    "        pos_sim = sim[i, i]\n",
    "\n",
    "        # remove true positive from negatives\n",
    "        neg_sims = sim[i].clone()\n",
    "        neg_sims[i] = -float(\"inf\")\n",
    "\n",
    "        # Select 2 hardest negatives\n",
    "        hard_idx = torch.topk(neg_sims, k=2).indices\n",
    "\n",
    "        z_p = z2[hard_idx[0]]\n",
    "        z_q = z2[hard_idx[1]]\n",
    "\n",
    "        # Mix the two negatives\n",
    "        lam = beta.sample().to(device)\n",
    "\n",
    "        z_mix = lam * z_p + (1.0 - lam) * z_q\n",
    "        z_mix = F.normalize(z_mix, dim=0)\n",
    "\n",
    "        mix_sim = torch.dot(z1[i], z_mix)\n",
    "\n",
    "        # Build denominator\n",
    "        denom = torch.exp(pos_sim / temperature)\n",
    "\n",
    "        # original batch negatives\n",
    "        denom = denom + torch.sum(\n",
    "            torch.exp(neg_sims / temperature)\n",
    "        )\n",
    "\n",
    "        # add synthetic negative\n",
    "        denom = denom + torch.exp(mix_sim / temperature)\n",
    "\n",
    "\n",
    "        # InfoNCE loss\n",
    "        total_loss += -torch.log(\n",
    "            torch.exp(pos_sim / temperature) / denom\n",
    "        )\n",
    "\n",
    "    return total_loss / N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab95cfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020 | Loss = 6.4316\n",
      "Epoch 040 | Loss = 6.2627\n",
      "Epoch 060 | Loss = 6.2109\n",
      "Epoch 080 | Loss = 6.1763\n",
      "Epoch 100 | Loss = 6.1510\n",
      "Epoch 120 | Loss = 6.1375\n",
      "Epoch 140 | Loss = 6.1235\n",
      "Epoch 160 | Loss = 6.1103\n",
      "Epoch 180 | Loss = 6.1012\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Optimize\u001b[39;00m\n\u001b[0;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 39\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\halac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    624\u001b[0m     )\n\u001b[1;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\halac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\halac\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    842\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    843\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder = GCNEncoder(num_feats).to(device)\n",
    "projector = ProjectionHead(128).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(projector.parameters()),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Training Loop (Graph SimCLR)\n",
    "\n",
    "epochs = 300\n",
    "temperature = 0.5\n",
    "\n",
    "encoder.train()\n",
    "projector.train()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "    # Two random augmented graph views\n",
    "    x1, edge1 = augment_graph(data)\n",
    "    x2, edge2 = augment_graph(data)\n",
    "\n",
    "    # Encode\n",
    "    h1 = encoder(x1, edge1)\n",
    "    h2 = encoder(x2, edge2)\n",
    "\n",
    "    # Project\n",
    "    z1 = projector(h1)\n",
    "    z2 = projector(h2)\n",
    "\n",
    "    # Contrastive loss\n",
    "    loss = mochi_loss(z1, z2, temperature)\n",
    "\n",
    "    # Optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch:03d} | Loss = {loss.item():.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2222d3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test accuracy: 0.7840\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = encoder(data.x, data.edge_index)\n",
    "\n",
    "# Train linear classifier on labeled train mask\n",
    "clf = nn.Linear(128, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(clf.parameters(), lr=0.01)\n",
    "\n",
    "for _ in range(200):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = clf(embeddings[data.train_mask])\n",
    "    loss = F.cross_entropy(out, data.y[data.train_mask])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "clf.eval()\n",
    "pred = clf(embeddings).argmax(dim=1)\n",
    "\n",
    "acc = (\n",
    "    pred[data.test_mask] ==\n",
    "    data.y[data.test_mask]\n",
    ").float().mean().item()\n",
    "\n",
    "print(f\"✅ Test accuracy: {acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
