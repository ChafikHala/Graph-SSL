{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99d5d645",
   "metadata": {},
   "source": [
    "#### MPNNs (Message-Passing Neural Networks) are at most as powerful as the 1-WL test in terms of distinguishing non-isomorphic graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80f3caa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C6 colors: Counter({0: 6})\n",
      "2xC3 colors: Counter({0: 6})\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from collections import Counter\n",
    "\n",
    "def wl_refinement(G, steps=5):\n",
    "    # initialize all colors to 0\n",
    "    colors = {v: 0 for v in G.nodes()}\n",
    "\n",
    "    for _ in range(steps):\n",
    "        new_colors = {}\n",
    "        for v in G.nodes():\n",
    "            neigh_colors = sorted(colors[u] for u in G.neighbors(v))\n",
    "            signature = (colors[v], tuple(neigh_colors))\n",
    "            new_colors[v] = hash(signature)\n",
    "\n",
    "        # compress hashes to small integers\n",
    "        unique = {c:i for i,c in enumerate(set(new_colors.values()))}\n",
    "        colors = {v: unique[new_colors[v]] for v in G.nodes()}\n",
    "\n",
    "    return Counter(colors.values())\n",
    "\n",
    "# G1: cycle of length 6\n",
    "G1 = nx.cycle_graph(6)\n",
    "\n",
    "# G2: two disjoint cycles of length 3\n",
    "C3 = nx.cycle_graph(3)\n",
    "G2 = nx.disjoint_union(C3, C3)\n",
    "\n",
    "\n",
    "\n",
    "print(\"C6 colors:\", wl_refinement(G1))\n",
    "print(\"2xC3 colors:\", wl_refinement(G2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecbec526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph embedding C6:    [0.10244211 0.         0.         0.55158514]\n",
      "Graph embedding 2xC3:  [0.10244211 0.         0.         0.55158514]\n",
      "Difference norm: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SimpleMPNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Dimension-preserving MPNN layer:\n",
    "    - mean aggregation\n",
    "    - linear + ReLU update\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        src, dst = edge_index  # [num_edges]\n",
    "\n",
    "        agg = torch.zeros_like(x)\n",
    "        agg.index_add_(0, dst, x[src])\n",
    "\n",
    "        deg = torch.zeros(x.size(0), device=x.device)\n",
    "        deg.index_add_(0, dst, torch.ones_like(dst, dtype=torch.float))\n",
    "        deg = deg.clamp(min=1.).unsqueeze(-1)\n",
    "\n",
    "        agg = agg / deg  # mean aggregation\n",
    "        return torch.relu(self.lin(agg))\n",
    "\n",
    "\n",
    "def to_edge_index(G: nx.Graph) -> torch.Tensor:\n",
    "    edges = []\n",
    "    for u, v in G.edges():\n",
    "        edges.append((u, v))\n",
    "        edges.append((v, u))  # undirected -> both directions\n",
    "    return torch.tensor(edges, dtype=torch.long).t()  # [2, num_edges]\n",
    "\n",
    "\n",
    "edge1 = to_edge_index(G1)\n",
    "edge2 = to_edge_index(G2)\n",
    "\n",
    "# ---------- 4. Run the same MPNN on both graphs ----------\n",
    "\n",
    "dim = 4\n",
    "gnn = SimpleMPNN(dim)\n",
    "\n",
    "# all nodes start with identical features\n",
    "x1 = torch.ones((G1.number_of_nodes(), dim))\n",
    "x2 = torch.ones((G2.number_of_nodes(), dim))\n",
    "\n",
    "h1, h2 = x1, x2\n",
    "\n",
    "num_layers = 4\n",
    "for _ in range(num_layers):\n",
    "    h1 = gnn(h1, edge1)\n",
    "    h2 = gnn(h2, edge2)\n",
    "\n",
    "# graph-level embeddings (mean pooling)\n",
    "g1_emb = h1.mean(dim=0)\n",
    "g2_emb = h2.mean(dim=0)\n",
    "\n",
    "print(\"Graph embedding C6:   \", g1_emb.detach().numpy())\n",
    "print(\"Graph embedding 2xC3: \", g2_emb.detach().numpy())\n",
    "print(\"Difference norm:\", torch.norm(g1_emb - g2_emb).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed086a",
   "metadata": {},
   "source": [
    "---\n",
    "### SimCLR\n",
    "\n",
    "##### 1. Overview\n",
    "\n",
    "**SimCLR (Simple Contrastive Learning of Representations)** is a self-supervised learning framework that trains neural networks to learn meaningful representations **without labels** by solving a *contrastive prediction task*.\n",
    "\n",
    "The key objective is:\n",
    "\n",
    "> Learn embeddings in which two augmented views of the **same sample** are close together, while embeddings of **different samples** are far apart.\n",
    "\n",
    "This is achieved using a contrastive loss known as **NT-Xent** (Normalized Temperature-scaled Cross-Entropy), which is a specific instantiation of the **InfoNCE** objective.\n",
    "\n",
    "\n",
    "\n",
    "##### 2. Data Pipeline\n",
    "\n",
    "For each data point $x \\sim \\mathcal{D}$:\n",
    "\n",
    "1. Two stochastic augmentations are sampled:\n",
    "   $$\n",
    "   x_i = t_a(x), \\qquad x_j = t_b(x).\n",
    "   $$\n",
    "\n",
    "2. Both views are passed through:\n",
    "   - an **encoder** $ f_\\theta $,\n",
    "   - a **projection head** $ g_\\phi $,\n",
    "\n",
    "   yielding:\n",
    "   $$\n",
    "   h = f_\\theta(x), \\qquad\n",
    "   z = g_\\phi(h), \\qquad\n",
    "   \\tilde z = \\frac{z}{\\|z\\|_2}.\n",
    "   $$\n",
    "\n",
    "3. $\\tilde z $ is the representation used for the contrastive loss.\n",
    "\n",
    "\n",
    "##### 3. Positive and Negative Pairs\n",
    "\n",
    "With a minibatch of size $ N $ original samples:\n",
    "\n",
    "- We produce $2N $ augmented views.\n",
    "- For each embedding $ \\tilde z_i $:\n",
    "  - the paired view $ \\tilde z_j $ of the same data sample is the **positive example**,\n",
    "  - the remaining $2N - 2 $ embeddings are treated as **negatives**.\n",
    "\n",
    "No explicit negative sampling is required — negatives come from the batch.\n",
    "\n",
    "\n",
    "\n",
    "##### 4. The NT-Xent (SimCLR) Loss\n",
    "\n",
    "The similarity function is cosine similarity with temperature scaling:\n",
    "\n",
    "$$\n",
    "s(\\tilde z_i, \\tilde z_k)\n",
    "=\n",
    "\\frac{\\tilde z_i^\\top \\tilde z_k}{\\tau},\n",
    "$$\n",
    "\n",
    "where $ \\tau > 0 $ is the temperature hyperparameter.\n",
    "\n",
    "For anchor  $i$ and its positive partner $ j $, the normalized temperature-scaled cross-entropy (**NT-Xent**) loss is:\n",
    "\n",
    "$$\n",
    "\\ell(i,j)\n",
    "=\n",
    "-\\log\n",
    "\\frac{\n",
    "\\exp\\!\\big(s(\\tilde z_i,\\tilde z_j)\\big)\n",
    "}{\n",
    "\\sum_{k=1}^{2N}\n",
    "\\mathbf{1}_{[k \\neq i]}\n",
    "\\exp\\!\\big(s(\\tilde z_i,\\tilde z_k)\\big)\n",
    "}.\n",
    "$$\n",
    "\n",
    "**Properties**\n",
    "\n",
    "- The **positive example is included in the denominator**.\n",
    "- The denominator forms a softmax over all candidates except the trivial self-pair $ i $.\n",
    "- Each embedding acts as an anchor once; the loss is **symmetrized**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{SimCLR}}\n",
    "=\n",
    "\\frac{1}{2N}\n",
    "\\sum_{(i,j)}\n",
    "\\big( \\ell(i,j) + \\ell(j,i) \\big).\n",
    "$$\n",
    "\n",
    "This is mathematically equivalent to performing a $2N-1$-way classification task:\n",
    "\n",
    "Given anchor $ i $, predict which candidate  $k \\neq i $ is its true positive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f28a561a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020 | Loss = 7.0897\n",
      "Epoch 040 | Loss = 6.9555\n",
      "Epoch 060 | Loss = 6.9072\n",
      "Epoch 080 | Loss = 6.8633\n",
      "Epoch 100 | Loss = 6.8404\n",
      "Epoch 120 | Loss = 6.8228\n",
      "Epoch 140 | Loss = 6.8099\n",
      "Epoch 160 | Loss = 6.7967\n",
      "Epoch 180 | Loss = 6.7903\n",
      "Epoch 200 | Loss = 6.7814\n",
      "Epoch 220 | Loss = 6.7783\n",
      "Epoch 240 | Loss = 6.7704\n",
      "Epoch 260 | Loss = 6.7603\n",
      "Epoch 280 | Loss = 6.7588\n",
      "Epoch 300 | Loss = 6.7526\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import dropout_edge\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "dataset = Planetoid(root=\"C:/Users/halac/Graph-SSL/data\", name=\"Cora\")\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "num_nodes  = data.num_nodes\n",
    "num_feats  = data.num_features\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "\n",
    "# Graph Augmentations (SimCLR views)\n",
    "\n",
    "\n",
    "def random_edge_dropout(edge_index, p=0.2):\n",
    "    edge_index, _ = dropout_edge(edge_index, p=p)\n",
    "    return edge_index\n",
    "\n",
    "def random_feature_mask(x, p=0.2):\n",
    "    mask = torch.rand_like(x) > p\n",
    "    return x * mask\n",
    "\n",
    "\n",
    "def augment_graph(data, edge_p=0.2, feat_p=0.2):\n",
    "    \"\"\"\n",
    "    Create a stochastic augmented view of the full graph.\n",
    "    \"\"\"\n",
    "    x = random_feature_mask(data.x, p=feat_p)\n",
    "    edge_index = random_edge_dropout(data.edge_index, p=edge_p)\n",
    "\n",
    "    return x, edge_index\n",
    "\n",
    "\n",
    "\n",
    "# GCN Encoder\n",
    "\n",
    "\n",
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=128, out_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(h, edge_index)\n",
    "        return h\n",
    "\n",
    "\n",
    "\n",
    "# Projection Head (SimCLR)\n",
    "\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, h):\n",
    "        return self.mlp(h)\n",
    "\n",
    "\n",
    "\n",
    "# NT-Xent Contrastive Loss (SimCLR)\n",
    "\n",
    "\n",
    "def nt_xent_loss(z1, z2, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Node-wise SimCLR loss.\n",
    "    \"\"\"\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "\n",
    "    N = z1.size(0)\n",
    "\n",
    "    z = torch.cat([z1, z2], dim=0)     # (2N, d)\n",
    "    sim = torch.matmul(z, z.T)         # cosine similarity\n",
    "\n",
    "    # positive pairs along diagonal\n",
    "    pos = torch.cat([\n",
    "        torch.diag(sim, N),\n",
    "        torch.diag(sim, -N)\n",
    "    ], dim=0)\n",
    "\n",
    "    neg_mask = ~torch.eye(2 * N, device=device, dtype=torch.bool)\n",
    "\n",
    "    negatives = sim[neg_mask].view(2 * N, -1)\n",
    "\n",
    "    logits = torch.cat([pos.unsqueeze(1), negatives], dim=1)\n",
    "    logits /= temperature\n",
    "\n",
    "    labels = torch.zeros(2 * N, dtype=torch.long, device=device)\n",
    "\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "\n",
    "\n",
    "# Model\n",
    "\n",
    "\n",
    "encoder = GCNEncoder(num_feats).to(device)\n",
    "projector = ProjectionHead(128).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(projector.parameters()),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Training Loop (Graph SimCLR)\n",
    "\n",
    "epochs = 300\n",
    "temperature = 0.5\n",
    "\n",
    "encoder.train()\n",
    "projector.train()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "    # Two random augmented graph views\n",
    "    x1, edge1 = augment_graph(data)\n",
    "    x2, edge2 = augment_graph(data)\n",
    "\n",
    "    # Encode\n",
    "    h1 = encoder(x1, edge1)\n",
    "    h2 = encoder(x2, edge2)\n",
    "\n",
    "    # Project\n",
    "    z1 = projector(h1)\n",
    "    z2 = projector(h2)\n",
    "\n",
    "    # Contrastive loss\n",
    "    loss = nt_xent_loss(z1, z2, temperature)\n",
    "\n",
    "    # Optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch:03d} | Loss = {loss.item():.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bd2fde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test accuracy: 0.7910\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = encoder(data.x, data.edge_index)\n",
    "\n",
    "# Train linear classifier on labeled train mask\n",
    "clf = nn.Linear(128, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(clf.parameters(), lr=0.01)\n",
    "\n",
    "for _ in range(200):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = clf(embeddings[data.train_mask])\n",
    "    loss = F.cross_entropy(out, data.y[data.train_mask])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "clf.eval()\n",
    "pred = clf(embeddings).argmax(dim=1)\n",
    "\n",
    "acc = (\n",
    "    pred[data.test_mask] ==\n",
    "    data.y[data.test_mask]\n",
    ").float().mean().item()\n",
    "\n",
    "print(f\"✅ Test accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d071762",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd872bc",
   "metadata": {},
   "source": [
    "## MixCo\n",
    "SimCLR is extended by introducing **mixup** and **soft positives**.\n",
    "\n",
    "#### Mixup\n",
    "\n",
    "For anchor $z_i$:\n",
    "\n",
    "1. Keep **true positive** $z_j$.\n",
    "2. Sample a random partner $z_q$.\n",
    "3. Sample a coefficient $\\lambda \\sim \\text{Beta}(\\alpha,\\alpha)$.\n",
    "4. Create a mixed representation:\n",
    "\n",
    "$z_{\\text{mix}} = \\lambda z_j + (1-\\lambda) z_q.$\n",
    "\n",
    "\n",
    "\n",
    "#### Soft targets\n",
    "\n",
    "The anchor now has **two positives instead of one**, with weights:\n",
    "\n",
    "$y_{ik} =\n",
    "\\begin{cases}\n",
    "\\lambda & k = j \\\\\n",
    "1 - \\lambda & k = q \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "All other embeddings remain negatives.\n",
    "\n",
    "\n",
    "\n",
    "#### Soft NT-Xent loss\n",
    "\n",
    "Using the same probabilities:\n",
    "\n",
    "$p_{ik} = \\frac{\\exp(s(z_i,z_k))}{\\sum_{m \\neq i} \\exp(s(z_i,z_m))},$\n",
    "\n",
    "the MixCo loss for anchor $i$ becomes:\n",
    "\n",
    "$L_i^{\\text{MixCo}}\n",
    "= -\\sum_k y_{ik}\\log p_{ik}.$\n",
    "\n",
    "\n",
    "\n",
    "#### Key differences\n",
    "\n",
    "| Method | Positives per anchor | Target type | Effect |\n",
    "|--------|------------------------|----------------|---------|\n",
    "| **SimCLR** | 1 (paired view) | Hard (0 or 1) | Strong pull to one positive, strong repulsion to all others |\n",
    "| **MixCo** | 2 (paired view + random mix partner) | Soft weights via $\\lambda$ | Reduced false-negative repulsion, smoother gradients, improved stability |\n",
    "\n",
    "\n",
    "\n",
    "#### About $\\lambda$\n",
    "\n",
    "- $\\lambda$ is **not fixed**.\n",
    "- For each mixed pair and each forward pass:\n",
    "\n",
    "$\\lambda \\sim \\text{Beta}(\\alpha,\\alpha)$\n",
    "\n",
    "- $\\alpha$ is the **hyperparameter** controlling the distribution:\n",
    "  - Larger $\\alpha$ → $\\lambda$ concentrates near $0.5$.\n",
    "  - Smaller $\\alpha$ → more extreme values (near 0 or 1).\n",
    "\n",
    "In practice $\\alpha \\approx 1$ or greater is used so that:\n",
    "\n",
    "- Very small $\\lambda$ values are rare.\n",
    "- The true positive usually retains significant weight.\n",
    "\n",
    "Occasional small $\\lambda$ acts as **benign regularization**, not destructive noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b714315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MixCo Loss (Graph MixCo)\n",
    "\n",
    "\n",
    "def mixco_loss(z1, z2, temperature=0.5, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Node-wise MixCo loss.\n",
    "    z1, z2: node embeddings from two augmented views\n",
    "    \"\"\"\n",
    "\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "\n",
    "    N, d = z1.shape\n",
    "\n",
    "    \n",
    "    # choose random mix partner for each node\n",
    "    \n",
    "    perm = torch.randperm(N, device=device)\n",
    "\n",
    "    z2_perm = z2[perm]\n",
    "\n",
    "    \n",
    "    # sample mixing coefficient lambda per node\n",
    "    \n",
    "    beta = torch.distributions.Beta(alpha, alpha)\n",
    "    lam = beta.sample((N,)).to(device)              # (N,)\n",
    "    lam = lam.view(N, 1)\n",
    "\n",
    "    \n",
    "    # create mixed positive representations\n",
    "    \n",
    "    z_mix = lam * z2 + (1. - lam) * z2_perm          # (N,d)\n",
    "\n",
    "    \n",
    "    # Step 4: similarity scores against all candidates\n",
    "    \n",
    "    z_all = torch.cat([z2, z2_perm], dim=0)          # (2N,d)\n",
    "\n",
    "    sim = torch.matmul(z_mix, z_all.T) / temperature  # (N,2N)\n",
    "\n",
    "    \n",
    "    # construct soft targets\n",
    "    # targets[i, j] = lambda_i\n",
    "    # targets[i, perm[i]+N] = (1-lambda_i)\n",
    "    \n",
    "    targets = torch.zeros(N, 2*N, device=device)\n",
    "\n",
    "    ij = torch.arange(N, device=device)\n",
    "\n",
    "    # true positives (z2)\n",
    "    targets[ij, ij] = lam.squeeze()\n",
    "\n",
    "    # mixed positives (permuted z2)\n",
    "    targets[ij, perm + N] = (1. - lam).squeeze()\n",
    "\n",
    "    # soft cross-entropy\n",
    "    log_probs = F.log_softmax(sim, dim=1)\n",
    "\n",
    "    loss = -(targets * log_probs).sum(dim=1).mean()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64bcef38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020 | Loss = 7.5789\n",
      "Epoch 040 | Loss = 7.3942\n",
      "Epoch 060 | Loss = 7.3576\n",
      "Epoch 080 | Loss = 7.3073\n",
      "Epoch 100 | Loss = 7.2677\n",
      "Epoch 120 | Loss = 7.3000\n",
      "Epoch 140 | Loss = 7.2399\n",
      "Epoch 160 | Loss = 7.3007\n",
      "Epoch 180 | Loss = 7.2183\n",
      "Epoch 200 | Loss = 7.2222\n",
      "Epoch 220 | Loss = 7.2636\n",
      "Epoch 240 | Loss = 7.2283\n",
      "Epoch 260 | Loss = 7.2715\n",
      "Epoch 280 | Loss = 7.2086\n",
      "Epoch 300 | Loss = 7.2205\n"
     ]
    }
   ],
   "source": [
    "encoder = GCNEncoder(num_feats).to(device)\n",
    "projector = ProjectionHead(128).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(projector.parameters()),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Training Loop (Graph MixCo)\n",
    "\n",
    "epochs = 300\n",
    "temperature = 0.2\n",
    "\n",
    "encoder.train()\n",
    "projector.train()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "    # Two random augmented graph views\n",
    "    x1, edge1 = augment_graph(data)\n",
    "    x2, edge2 = augment_graph(data)\n",
    "\n",
    "    # Encode\n",
    "    h1 = encoder(x1, edge1)\n",
    "    h2 = encoder(x2, edge2)\n",
    "\n",
    "    # Project\n",
    "    z1 = projector(h1)\n",
    "    z2 = projector(h2)\n",
    "\n",
    "    # Contrastive loss\n",
    "    loss = mixco_loss(z1, z2, temperature, alpha = 2.0)\n",
    "\n",
    "    # Optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch:03d} | Loss = {loss.item():.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "585d60fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test accuracy: 0.6930\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = encoder(data.x, data.edge_index)\n",
    "\n",
    "# Train linear classifier on labeled train mask\n",
    "clf = nn.Linear(128, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(clf.parameters(), lr=0.01)\n",
    "\n",
    "for _ in range(200):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = clf(embeddings[data.train_mask])\n",
    "    loss = F.cross_entropy(out, data.y[data.train_mask])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "clf.eval()\n",
    "pred = clf(embeddings).argmax(dim=1)\n",
    "\n",
    "acc = (\n",
    "    pred[data.test_mask] ==\n",
    "    data.y[data.test_mask]\n",
    ").float().mean().item()\n",
    "\n",
    "print(f\"✅ Test accuracy: {acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
